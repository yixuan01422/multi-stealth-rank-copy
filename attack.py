import torch, random, unicodedata
from tqdm import tqdm
import torch.nn as nn
import torch.nn.functional as F
from process import greedy_decode, greedy_decode_skip_special, process_headtail, select_topk, create_word_mask, mask_logits, get_logits_embedding, print_topk_predictions
from transformers import DynamicCache
# from colorama import Fore, Style
# mostly copied from previous project StealthRank

def soft_forward(model, head_tokens, prompt_logits, 
                   tail_tokens, target_tokens=None, tokenizer=None):
    embedding_layer = model.get_input_embeddings()  # Usually model.embeddings or model.get_input_embeddings()
    
    # Step 2: Hard-select embeddings for head and tail
    head_embeddings =  embedding_layer(head_tokens)  # Shape: (batch_size, head_length, embedding_dim)
    tail_embeddings = embedding_layer(tail_tokens)  # Shape: (batch_size, tail_length, embedding_dim)

    # Step 3: Soft-select embeddings for prompt
    prompt_probs = torch.softmax(prompt_logits, dim=-1)  # Shape: (batch_size, prompt_length, vocab_size)
    vocab_embeddings = embedding_layer.weight  # Embedding matrix (vocab_size, embedding_dim)
    prompt_embeddings = torch.matmul(prompt_probs, vocab_embeddings)  # Shape: (batch_size, prompt_length, embedding_dim)

    total = [head_embeddings, prompt_embeddings, tail_embeddings]

    start = head_tokens.shape[1] - 1
    end = start + prompt_logits.shape[1]

    # Step 4: add target embeddings if provided
    if target_tokens is not None:
        target_embeddings = embedding_layer(target_tokens)  # Shape: (batch_size, target_length, embedding_dim)
        total.append(target_embeddings)
        start = -1 - target_tokens.shape[1]
        end = -1

    sequence_embeddings = torch.cat(total, dim=1)

    # Step 5: Forward pass through the model
    logits = model(inputs_embeds=sequence_embeddings).logits
    # return the prompt logits
    specific_logits = logits[:, start:end, :]
    # if target_tokens is not None:
    #     decoded_tokens, decoded_text = greedy_decode(specific_logits, tokenizer)
    #     print("decoded_text:", decoded_text)
    return specific_logits


def fluency_loss(logits_perturbed, logits):
    p = F.softmax(logits_perturbed, dim=-1)
    logp = F.log_softmax(logits, dim=-1)
    return -(p * logp).sum(dim=-1).mean(dim=-1)


def bleu_loss(decoder_outputs, target_idx, ngram_list, pad=0, weight_list=None):
    batch_size, output_len, _ = decoder_outputs.size()
    _, tgt_len = target_idx.size()
    if type(ngram_list) == int:
        ngram_list = [ngram_list]
    if ngram_list[0] <= 0:
        ngram_list[0] = output_len
    if weight_list is None:
        weight_list = [1. / len(ngram_list)] * len(ngram_list)
    decoder_outputs = torch.log_softmax(decoder_outputs,dim=-1)
    decoder_outputs = torch.relu(decoder_outputs + 20) - 20
    # target_idx expand to batch size * tgt len
    target_idx = target_idx.expand(batch_size, -1)
    index = target_idx.unsqueeze(1).expand(-1, output_len, tgt_len)
    cost_nll = decoder_outputs.gather(dim=2, index=index)
    cost_nll = cost_nll.unsqueeze(1)
    out = cost_nll
    sum_gram = 0. 

    zero = torch.tensor(0.0).to(decoder_outputs.device)
    target_expand = target_idx.view(batch_size,1,1,-1).expand(-1,-1,output_len,-1)
    out = torch.where(target_expand==pad, zero, out)

    for cnt, ngram in enumerate(ngram_list):
        if ngram > output_len:
            continue
        eye_filter = torch.eye(ngram).view([1, 1, ngram, ngram]).to(decoder_outputs.device)
        term = nn.functional.conv2d(out, eye_filter)/ngram
        if ngram < decoder_outputs.size()[1]:
            term = term.squeeze(1)
            gum_tmp = F.gumbel_softmax(term, tau=1, dim=1)
            term = term.mul(gum_tmp).sum(1).mean(1)
        else:
            while len(term.shape) > 1:
                assert term.shape[-1] == 1, str(term.shape)
                term = term.sum(-1)
        sum_gram += weight_list[cnt] * term

    loss = - sum_gram
    return loss






def add_static_noise(prompt_logits, iter, iter_steps, noise_stds):

    def get_noise_std(iteration):
        for i, step in enumerate(iter_steps):
            if iteration < step:
                return noise_stds[i - 1] if i > 0 else noise_stds[0]
        return noise_stds[-1]
            
    noise_std = get_noise_std(iter)

    noise = torch.normal(mean=0.0, std=noise_std, size=prompt_logits.size(),
                        device=prompt_logits.device, requires_grad=False)
    
    return prompt_logits + noise


def rank_products(text, product_names):
    position_dict = {}
    for name in product_names:
        position = text.find(name)
        if position != -1:
            position_dict[name] = position
        else:
            position_dict[name] = float('inf')

    # Sort products by position
    sorted_products = sorted(position_dict, key=position_dict.get)

    ranks = {}
    for i, prod in enumerate(sorted_products):
        if position_dict[prod] != float('inf'):
            ranks[prod] = i + 1
        else:
            ranks[prod] = len(sorted_products) + 1

    return ranks


# def print_iteration_metrics(iteration, total_loss, fluency_loss, n_gram_loss, target_loss):
#     log_message = (
#         f"{Fore.GREEN}Iteration {iteration+1}: {Style.RESET_ALL}"
#         f"{Fore.YELLOW}Total Loss: {total_loss:.4f} {Style.RESET_ALL}"
#         f"{Fore.BLUE}Fluency Loss: {fluency_loss:.4f} {Style.RESET_ALL}"
#         f"{Fore.CYAN}N-gram Loss: {n_gram_loss:.4f} {Style.RESET_ALL}"
#         f"{Fore.MAGENTA}Target Loss: {target_loss:.4f} {Style.RESET_ALL}"
#     )
#     print(f"{log_message}", flush=True)

def topk_decode(model, tokenizer, y_logits, topk, temperature, head_tokens):
    # empty cache
    prompt_length = y_logits.shape[1]

    embedding_layer = model.get_input_embeddings()
    
    head_embed = embedding_layer(head_tokens)

    # to get past key values for efficient decoding
    past_embed = head_embed[:, :-1, :]
    past = model(inputs_embeds=past_embed, use_cache=True).past_key_values

    input_embeds = head_embed[:, -1:, :]

    for i in range(prompt_length):
        output = model(inputs_embeds=input_embeds, past_key_values=past, use_cache=True)
        past = output.past_key_values
        last_logits = output.logits[:, -1:, :]
        topk_mask = select_topk(last_logits, topk)

        total_topk_mask = topk_mask if i == 0 else torch.cat([total_topk_mask, topk_mask], dim=1)

        if i < prompt_length - 1:
            current_y_logits = mask_logits(y_logits[:, i:i+1, :], topk_mask) / temperature
            input_embeds = torch.matmul(F.softmax(current_y_logits, dim=-1).to(embedding_layer.weight.dtype),
                                         embedding_layer.weight)

    complete_masked_logits = mask_logits(y_logits, total_topk_mask)

    decoded_tokens, decoded_text = greedy_decode(complete_masked_logits, tokenizer)

    return decoded_tokens, decoded_text, complete_masked_logits


def gcg_decode(model, tokenizer, head_tokens, y_logits, topk, tail_tokens, target_tokens):
    # apply one step of greedy coordinate gradient focusing only on target loss
    embedding_layer = model.get_input_embeddings()
    embedding_matrix = embedding_layer.weight.data

    head_embeddings = embedding_layer(head_tokens)
    tail_embeddings = embedding_layer(tail_tokens)
    target_embeddings = embedding_layer(target_tokens)

    prompt_probs = torch.softmax(y_logits, dim=-1)
    prompt_embeddings = torch.matmul(prompt_probs, embedding_matrix)
    input_embeddings = torch.cat([head_embeddings, prompt_embeddings, tail_embeddings], dim=1)
    input_embeddings.retain_grad()

    total_embeddings = torch.cat([input_embeddings, target_embeddings], dim=1)
    
    logits = model(inputs_embeds=total_embeddings).logits

    loss_fn = nn.CrossEntropyLoss()

    loss = []

    for i in range(input_embeddings.shape[0]):
        loss.append(loss_fn(logits[i, input_embeddings.shape[1]-1:-1, :], target_tokens[i]))

    loss = torch.stack(loss).sum()

    (-loss).backward()
    gradients = input_embeddings.grad

    start = head_tokens.shape[1] - 1
    end = start + y_logits.shape[1]

    dot_prod = torch.matmul(gradients, embedding_matrix.T)[:, start:end, :]

    topk_mask = select_topk(y_logits, topk)

    # mask the dot_prod
    dot_prod = mask_logits(dot_prod, topk_mask)

    decoded_tokens, decoded_text = greedy_decode(dot_prod, tokenizer)

    return decoded_tokens, decoded_text



def log_result(model, tokenizer, head_tokens, logits, tail_tokens, target_tokens,
               iter, product_list, target_product, topk, temperature):
    
    product_names = [product['Name'] for product in product_list]

    # prompt_tokens, decoded_prompt, _ = topk_decode(model, tokenizer, logits, topk, temperature, head_tokens)
    # use greedy decode
    prompt_tokens, decoded_prompt = greedy_decode(logits, tokenizer)

    # use gcg decode
    # with torch.autocast(device_type=model.device.type, dtype=torch.float16):
    #     prompt_tokens, decoded_prompt = gcg_decode(model, tokenizer, head_tokens, logits, topk, tail_tokens, target_tokens)

    # Concatenate head prompt, decoded text, and tail tokens
    complete_prompt = torch.cat([head_tokens, prompt_tokens, tail_tokens], dim=1)

    # Generate result from model using the complete prompt
    batch_result = model.generate(complete_prompt, 
                                  max_new_tokens=500, 
                                #   do_sample=True, temperature=0.7, top_k=topk, # NOT sure if we need this 2 parameters. used in cold-attack but not in product ranking
                                  attention_mask=torch.ones_like(complete_prompt), 
                                  pad_token_id=tokenizer.eos_token_id)

    # index batch_result to avoid the input prompt
    batch_result = batch_result[:, complete_prompt.shape[1]:]

    generated_texts = tokenizer.batch_decode(batch_result, skip_special_tokens=True)

    product_ranks = []
    
    # Log the complete prompt and the generated result
    for i in range(len(decoded_prompt)):
        current_rank = rank_products(generated_texts[i], product_names)[target_product]
        product_ranks.append(current_rank)
        highlighted_attack_prompt = f'<span style="color:red;">{decoded_prompt[i]}</span>'
        current_complete_prompt = tokenizer.decode(complete_prompt[i], skip_special_tokens=True)

        log_entry = {
            "attack_prompt": highlighted_attack_prompt,
            "complete_prompt": current_complete_prompt.replace(decoded_prompt[i], highlighted_attack_prompt),
            "generated_result": generated_texts[i],
            'product_rank': current_rank
        }
        
        print(f"Iteration {iter}: Target Product: {target_product}, Rank: {current_rank}")

    print(f"Target Product: {target_product} Rank: {min(product_ranks)} (higher means worse rank)")
    
    return min(product_ranks)



def soft_log_result(model, tokenizer, head_tokens, logits, tail_tokens, 
               iter, product_list, target_product, topk, temperature):
    torch.cuda.empty_cache()
    product_names = [product['Name'] for product in product_list]

    # _, _, masked_logits = topk_decode(model, tokenizer, logits, topk, temperature, head_tokens)

    embedding_layer = model.get_input_embeddings()  # Usually model.embeddings or model.get_input_embeddings()
    
    # Step 2: Hard-select embeddings for head and tail
    head_embeddings =  embedding_layer(head_tokens)  # Shape: (batch_size, head_length, embedding_dim)
    tail_embeddings = embedding_layer(tail_tokens)  # Shape: (batch_size, tail_length, embedding_dim)
    # Step 3: Soft-select embeddings for prompt
    # prompt_probs = torch.softmax(masked_logits, dim=-1)  # Shape: (batch_size, prompt_length, vocab_size)
    # vocab_embeddings = embedding_layer.weight  # Embedding matrix (vocab_size, embedding_dim)

    # prompt_embeddings = torch.matmul(prompt_probs, vocab_embeddings)  # Shape: (batch_size, prompt_length, embedding_dim)

    # hard select prompt embeddings
    prompt_embeddings = embedding_layer(logits.argmax(dim=-1))

    total_embeddings = torch.cat([head_embeddings, prompt_embeddings, tail_embeddings], dim=1)

    result = []
    past_embed = total_embeddings[:, :-1, :]
    past = model(inputs_embeds=past_embed, use_cache=True).past_key_values
    past = DynamicCache.from_legacy_cache(past)

    input_embeds = total_embeddings[:, -1:, :]

    for i in range(800-total_embeddings.shape[1]):
        output = model(inputs_embeds=input_embeds, past_key_values=past, use_cache=True)
        past = DynamicCache.from_legacy_cache(output.past_key_values)
        last_logits = output.logits[:, -1:, :]

        token = torch.argmax(last_logits, dim=-1)
        result.append(token)
        input_embeds = model.get_input_embeddings()(token)

    batch_result = torch.cat(result, dim=1)
    generated_texts = tokenizer.batch_decode(batch_result, skip_special_tokens=True)

    product_ranks = []
    
    # Log the complete prompt and the generated result
    for i in range(total_embeddings.shape[0]):
        current_rank = rank_products(generated_texts[i], product_names)[target_product]
        product_ranks.append(current_rank)
        # decode head and tail separately
        current_head = tokenizer.decode(head_tokens[i], skip_special_tokens=True)
        current_tail = tokenizer.decode(tail_tokens[i], skip_special_tokens=True)

        log_entry = {
            "attack_prompt": 'soft',
            "complete_prompt": current_head + '<span style="color:red;">soft</span>' + current_tail,
            "generated_result": generated_texts[i],
            'product_rank': current_rank
        }
        
        print(f"Soft Iteration {iter}: Target Product: {target_product}, Rank: {current_rank}")

    print(f"Soft Target Product: {target_product} Rank: {min(product_ranks)} (higher means worse rank)")
    
    return min(product_ranks)


def attack_control(model, tokenizer, system_prompt, user_msg,
                   prompt_logits, target_tokens, extra_word_tokens, 
                   product_list, target_product,
                   original_embedding=None, **kwargs):

    mode = kwargs['mode'] # suffix or paraphrase
    device = model.device
    epsilon = nn.Parameter(torch.zeros_like(prompt_logits))
    
    # Variable to store the final decoded prompt
    final_decoded_prompt = None
    # Record best loss and corresponding y_logits
    best_loss = float('inf')
    best_y_logits = None
    # kaiming initialize  
    # nn.init.kaiming_normal_(epsilon)
    optimizer = torch.optim.Adam([epsilon], lr=kwargs['lr'])
    print(f"ðŸŽ¯ Optimizer initialized with lr={kwargs['lr']}")
    batch_size = prompt_logits.size(0)

    extra_mask = create_word_mask(extra_word_tokens, prompt_logits)
    
    topk_mask = None
    current_rank = None

    with tqdm(total=kwargs['num_iter'], desc="Training", unit="iter", disable=True) as pbar:
        for iter in range(kwargs['num_iter']):  
            if kwargs['random_order']:
                # shuffle the product list
                random.shuffle(product_list)

            head_tokens, tail_tokens = process_headtail(tokenizer, system_prompt, product_list, user_msg, 
                                                        target_product, batch_size, mode, device, last=False)
            # add learnable noise to prompt logits
            y_logits = prompt_logits + epsilon

            # fluency loss
            if topk_mask is None:
                fluency_soft_logits = (y_logits.detach() / kwargs['temperature'] - y_logits).detach() + y_logits
            else:
                fluency_soft_logits = mask_logits(y_logits, topk_mask, extra_mask) / kwargs['temperature']

            with torch.autocast(device_type=device.type, dtype=eval(f"torch.bfloat{kwargs['precision']}")):
                perturbed_y_logits = soft_forward(model, head_tokens, fluency_soft_logits, tail_tokens).detach()
            
            topk_mask = select_topk(perturbed_y_logits, kwargs['topk'])

            perturbed_y_logits = mask_logits(perturbed_y_logits, topk_mask, extra_mask)

            flu_loss = fluency_loss(perturbed_y_logits, y_logits)
            
            # target loss
            # soft_logits = y_logits / kwargs['temperature']  
            soft_logits = (y_logits.detach() / kwargs['temperature'] - y_logits).detach() + y_logits
            with torch.autocast(device_type=device.type, dtype=eval(f"torch.bfloat{kwargs['precision']}")):
                target_logits = soft_forward(model, head_tokens, soft_logits, tail_tokens, target_tokens, tokenizer)
                
            target_loss = nn.CrossEntropyLoss(reduction='none')(
                target_logits.reshape(-1, target_logits.size(-1)), 
                target_tokens.view(-1))
            target_loss = target_loss.view(batch_size, -1).mean(dim=-1)
            
            # ðŸ” Tokençº§åˆ«é¢„æµ‹åˆ†æž
            if iter == 0 or iter == kwargs['num_iter'] - 1:  
                with torch.no_grad():
                    # èŽ·å–æ¯ä¸ªä½ç½®target tokençš„é¢„æµ‹æ¦‚çŽ‡
                    target_probs = torch.softmax(target_logits, dim=-1)  # [batch_size, target_length, vocab_size]
                    target_token_probs = torch.gather(target_probs, dim=-1, index=target_tokens.unsqueeze(-1)).squeeze(-1)
                    
                    # è§£ç target tokensæ¥æ˜¾ç¤ºæ¯ä¸ªtokençš„å†…å®¹
                    target_tokens_list = target_tokens[0].tolist()  # ç¬¬ä¸€ä¸ªbatch
                    target_probs_list = target_token_probs[0].tolist()  # ç¬¬ä¸€ä¸ªbatchçš„æ¦‚çŽ‡
                    
                    print(f"\nðŸŽ¯ Target Token Analysis (Iter {iter}):")
                    print(f"Target string: {tokenizer.decode(target_tokens_list, skip_special_tokens=False)}")
                    print("Token-wise prediction probabilities:")
                    
                    for pos, (token_id, prob) in enumerate(zip(target_tokens_list, target_probs_list)):
                        token_text = tokenizer.decode([token_id], skip_special_tokens=False)
                        print(f"  Position {pos}: '{token_text}' (ID: {token_id}) -> Probability: {prob:.4f}")

                    print("-" * 60)

            # ngram bleu loss
            if mode == 'suffix':
                n_gram_loss = bleu_loss(y_logits, extra_word_tokens, ngram_list=[1])
                total_loss = kwargs['fluency'] * flu_loss - kwargs['ngram'] * n_gram_loss + kwargs['target'] * target_loss
                #total_loss = kwargs['target'] * target_loss
                similarity_score = None

            elif mode == 'paraphrase':
                mask_y_logits = mask_logits(y_logits, topk_mask, extra_mask)
                n_gram_loss = bleu_loss(mask_y_logits, extra_word_tokens, ngram_list=[1, 2, 3])

                logits_embedding = get_logits_embedding(model, y_logits, kwargs['temperature'])

                similarity_score = F.cosine_similarity(original_embedding, logits_embedding)
                total_loss = kwargs['fluency'] * flu_loss + kwargs['ngram'] * n_gram_loss + kwargs['target'] * target_loss - kwargs['similarity'] * similarity_score

            
            # total loss weighted sum
            
            total_loss = total_loss.mean()
            
            # Update best result
            current_loss = total_loss.item()
            if current_loss < best_loss:
                best_loss = current_loss
                best_y_logits = y_logits.detach().clone()
            
            pbar.set_postfix({
                "Total Loss": f"{total_loss.item():.2f}",
                "Target": f"{target_loss.mean().item():.2f}",
                "Fluency": f"{flu_loss.mean().item():.2f}",
                "N-gram": f"{n_gram_loss.mean().item():.2f}",
                "Similarity": f"{similarity_score.mean().item():.2f}" if similarity_score is not None else "N/A"
            })
            

            # Training loss logging
            if iter % 10 == 0:  # Log every 10 iterations
                print(f"Step {iter}: Loss={total_loss.item():.4f}, "
                      f"Fluency={flu_loss.mean().item():.4f}, "
                      f"N-gram={n_gram_loss.mean().item():.4f}, "
                      f"Target={target_loss.mean().item():.4f}")
                if similarity_score is not None:
                    print(f"Similarity={similarity_score.mean().item():.4f}")


            # evaluate and log into a jsonl file
            if iter == 0 or (iter+1) % kwargs['test_iter'] == 0 or iter == kwargs['num_iter'] - 1:
                # with torch.autocast(device_type=device.type, dtype=eval(f"torch.float{kwargs['precision']}")), torch.no_grad():
                #     table, rank = soft_log_result(model, tokenizer, head_tokens, y_logits, tail_tokens, 
                #             iter, product_list, target_product, table, logger, kwargs['topk'], kwargs['temperature'])
                    
                # inference at random places
                if kwargs['random_inference']:
                    head_tokens, tail_tokens = process_headtail(tokenizer, system_prompt, product_list, user_msg, 
                                                        target_product, batch_size, mode, device, last=False)

                # Skip log_result to avoid gradient computation graph conflicts
                # rank = log_result(model, tokenizer, head_tokens, y_logits, tail_tokens, target_tokens,
                #                 iter, product_list, target_product, kwargs['topk'], kwargs['temperature'])
                
                # Training completed message on the final iteration
                if iter == kwargs['num_iter'] - 1:
                    final_decoded_prompt_list, final_selected_ids_list = greedy_decode_skip_special(y_logits, tokenizer)
                    final_decoded_prompt = final_decoded_prompt_list[0] if isinstance(final_decoded_prompt_list, list) else final_decoded_prompt_list
                    final_selected_ids = final_selected_ids_list[0] if isinstance(final_selected_ids_list, list) else final_selected_ids_list
                    tqdm.write(f"Final adversarial prompt generated at iteration {iter + 1}")
                    
                    # Print Top-3 predictions for each position
                    # print_topk_predictions(y_logits, tokenizer, k=3)
            pbar.update(1)
            
            # add static noise and do not add either static or learnable noise at the last iteration
            if iter < kwargs['num_iter'] - 1:
                optimizer.zero_grad()
                total_loss.backward()
                optimizer.step()
                prompt_logits = add_static_noise(prompt_logits, iter, kwargs['iter_steps'], kwargs['noise_stds'])
        
    # Decode the best y_logits after training
    best_selected_ids = None
    if best_y_logits is not None:
        best_decoded_prompt_list, best_selected_ids_list = greedy_decode_skip_special(best_y_logits, tokenizer)
        best_decoded_prompt = best_decoded_prompt_list[0] if isinstance(best_decoded_prompt_list, list) else best_decoded_prompt_list
        best_selected_ids = best_selected_ids_list[0] if isinstance(best_selected_ids_list, list) else best_selected_ids_list
    else:
        best_decoded_prompt = None
    
    # Return decoded prompt, selected_ids, and the last y_logits (for multi-round optimization)
    # Can return either: final_decoded_prompt (last iteration) or best_decoded_prompt (lowest loss)
    returned_prompt = final_decoded_prompt if final_decoded_prompt else None
    returned_selected_ids = final_selected_ids if final_decoded_prompt else best_selected_ids
    return best_decoded_prompt, best_selected_ids, best_y_logits
    #return returned_prompt, returned_selected_ids, y_logits
    


    
